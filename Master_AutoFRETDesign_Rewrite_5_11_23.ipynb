{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#script may throw some warning from Panda about writing to dataframes. This is because Pandas very cautiously warns you any\n",
    "#time you ever write to a copy of a dataframe, even if you then store it somewhere, since many of its operations on slices\n",
    "#are not \"in-place\" or may not act as they are expected to for experienced numpy users\n",
    "\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "#script incorporates fix on our side for frettraj issue so frettraj doesnt need to be edited\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVs Directory exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To implement:\n",
    "Move everything into pd DataFrames\n",
    "    May require multiple for data of different dimensions\n",
    "        1D: sequence-wise multi-column, 2D: pair-wise multi-column, larger 2D: pairs of pairs multi-column (could also be 4d multi column for compressed dimensions) \n",
    "Add multi-chain support\n",
    "Move Parameters to external settings file\n",
    "    Step 1: move all settings to a dictionary or class\n",
    "Implement automatic performance of MSA for DCA\n",
    "Implement reloading of AV data\n",
    "Implement optional use of frettraj\n",
    "\"\"\"\n",
    "\n",
    "###########################\n",
    "### 1. General settings ###\n",
    "###########################\n",
    "\n",
    "settings={}\n",
    "settings['work_directory'] = \\\n",
    "    r'/zfs/smblab//fduffy/source/repos/FRETNet-Designer/Test/'      # The directory containing the data and structures of interest. Outputs of this code will be saved in subfolders.\n",
    "settings['output_directory']=settings['work_directory']+'outputs/'  # Subfolder to save output files in. Will be created if it doesn't exist\n",
    "settings['input_directory']=settings['work_directory']+'inputs/'    # name of a subfolder from a previous run from which things like AVs can be reloaded rather than re-calculated\n",
    "settings['dependancy_directory'] = \\\n",
    "    r'/zfs/smblab//fduffy/source/repos/FRETNet-Designer/Script_Dependancies' # Directory containing the Python implementation of DCA and other custom functions\n",
    "settings['input_structures'] = \\\n",
    "    [\"RBP_closed_fromSBM+DCA.pdb\", \"RBP_open_fromSBM+DCA.pdb\", \"RBP_Twisted_allatom.pdb\"] # Input structure files, must be list of strings which are names of files\n",
    "settings['fretraj_fix_our_side'] = True # Switch for whether we want to use logic which fixes the fretraj serial bug on our side or not. Previously we would hand-modify their package for a new install.\n",
    "settings['Randomization Seed']=10\n",
    "\n",
    "\n",
    "###########################\n",
    "### 1.1 Figure settings ###\n",
    "###########################\n",
    "\n",
    "settings['figure_parameters']={'legend.fontsize': 'xx-large',\n",
    "         'figure.figsize': (10, 10),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large',\n",
    "         'lines.linewidth':1,\n",
    "         'font.family':'Arial',\n",
    "         'ps.fonttype':42,\n",
    "         'pdf.fonttype':42}\n",
    "\n",
    "#series of settings for whether or not to do a step, or whether to load a previous run or re-perform the action of a module\\\n",
    "#these accept \"False\" (F), \"True\" (T), and \"Load\" (L) for ignoring, performing, or loading a previous run, respectively\n",
    "#Some parameters accept \"Specify\" (S), an option like load, but it lets you point to a specific folder/set of files/file to load that was pre-generated, rather than using \n",
    "#all outputs from the folder specified in \"input\"\n",
    "settings['do_make_MSA']=True #T/F/L/S\n",
    "settings['do_DCA']=True #T/F/L/S\n",
    "settings['do_conservation_score']=True #T/F/L/S\n",
    "settings['Do_AV']=True#'Load' #True or 'Load' or 'Specify'\n",
    "\n",
    "##########################################\n",
    "### 2. Domain and Residue Restrictions ###\n",
    "##########################################\n",
    "\n",
    "settings['define_domains'] = True                       # defines domains between which sites should be tested. This prevents sites from within the same domains \n",
    "                                                        # being tested against one another. this essentially is a time saver, as sites within separate domains \n",
    "                                                        # which are mutually re-orienting are likely to provide higher contrast than sites within a single domain\n",
    "settings['domain_bounds'] = \\\n",
    "        [['Dom1',1,103], ['Dom2',104,271]]               # domain bounds accepts a list of lists containing the domain names, start residue, and end residue (in terms \n",
    "                                                         # of the input file indexing). \n",
    "                                                         # ['Dom3', 235, 271]] #takes name of domain plus the residue limits in terms of indices loaded by mdtraj\n",
    "settings['exclude_residues'] = False                     # exluded residues allows particular residues to be excluded. This will remove the residue from any relevant lists if present,\n",
    "                                                         # but will not perturb the domain definitions,\n",
    "                                                         # ranges of residues can be specified to be excluded as a list of [start, stop] inclusive. these will be popped out and \n",
    "                                                         # generate individual entries into the exclusion list, so this is only for convenience. These are ignorby some analyses, \n",
    "                                                         # and those analyses will be performed across the whole sequence. excluded really just marks these sitesed \"bad\" as if \n",
    "                                                         # some parameter eliminated them, such as low SASA.\n",
    "                                                         # domain bounds really works at the pair level.\n",
    "settings['excluded_residues'] = [61,151,[152,155], 246]  # in this example, first two entries are redundant with domain bounds, 3rd eliminates a site in domain 2. Currently must feed this a list (even if empty)\n",
    "if settings['exclude_residues']:\n",
    "    for i in settings['excluded_residues']:\n",
    "        if type(i)==int:\n",
    "            continue\n",
    "        if type(i)==list:\n",
    "            settings['excluded_residues'].remove(i)\n",
    "            settings['excluded_residues'].extend(list(range(i[0],i[1]+1)))\n",
    "        else:\n",
    "            raise Exception('Invalid index encountered in exclusion list.')\n",
    "    settings['excluded_residues'].sort()\n",
    "\n",
    "############################  \n",
    "### 3. MSA file settings ###\n",
    "############################\n",
    "\n",
    "settings['generate_MSA']=True   # Whether to generate an MSA during run-time or the user will provide a pre-generated MSA file. \n",
    "                                # Must ensure correct formating for DCA. Too many MSA formats to account for all of them. \n",
    "                                # Sequences should be padded to align exactly along the input structure sequence using '-' placeholder character.\n",
    "                                # Each sequence should appear on a single line, with lines separated by >header lines.\n",
    "\n",
    "settings['pregenerated_MSA_file'] = \\\n",
    "        settings['input_directory']+'OutMSA_filtered30.msa'    #location of pregenerated MSA file if generate_MSA == False\n",
    "\n",
    "###################################################\n",
    "### 3.1 settings for MSA generation at run-time ###\n",
    "###################################################\n",
    "\n",
    "settings['input_seed'] = \\\n",
    "    \"Input_Structure_FASTA.fasta\"                                       # location of input seed sequence file\n",
    "settings['max_align_gaps'] = \"30\"                                               # max number allowed gaps in sequences to allow for in filtering\n",
    "settings['outputHMM'] = 'OutHMM.hmm'                                          # ouput HMM file to generate.\n",
    "settings['database_path'] = \\\n",
    "    '/zfs/smblab/group_software/HMMER/db/Pfam-A.fasta.gz'                     # database location\n",
    "settings['outputMSA_filename'] = 'OutMSA.msa'                                 # creates full sequence file with this name as well as filtered with this name +'_filtered<gapnumber>'\n",
    "settings['hmmer_bin'] = '/zfs/smblab/group_software/HMMER/install/bin/'       # location of hmmer bin for accessing hmmer CLI tools\n",
    "settings['easel_miniapps'] = \\\n",
    "        '/zfs/smblab/group_software/HMMER/hmmer-3.3.2/easel/miniapps/'        # location of easel miniapps for access to easel CLI tools\n",
    "settings['filter_pfam_path'] = \\\n",
    "        '/zfs/smblab/fduffy/source/repos/FRETNet-Designer/filter_pfam_args.py' #filter pfam script path\n",
    "\n",
    "#######################\n",
    "### 4. DCA settings ###\n",
    "#######################\n",
    "\n",
    "settings['pregenerated_DCA']=True                                            # whether DCA was pre-run. if true, need to specify a DCA output file\n",
    "settings['pregenerated_DCA_file']=settings['output_directory']+'outDCA.dat'\n",
    "settings['DCA_output']=settings['output_directory']+'outDCA.dat'\n",
    "\n",
    "###############################\n",
    "### 5. SASA and AV Settings ###\n",
    "###############################\n",
    "\n",
    "settings['contact_cutoff']=.5 #nm. cutoff to convert residue to \"bad\" residue based on inter-residue contacts. \n",
    "settings['SASA_cutoff']=0.2 #nm^2 cutoff below which the low SASA of the residue disqualifies if from labeling\n",
    "settings['contact_cutoff_mode']='average' #average or individual. Decides whether to check against the average contact or all individually (if all individually, any 1 below threshold discards the residue)\n",
    "settings['SASA_cutoff_mode']='average' #as above for contacts\n",
    "\n",
    "settings['generate_dye_parameters_file']=True #True/Load/Specify (need dye parameters for AV sims)\n",
    "settings['specified_dye_parameters_file']=None\n",
    "\n",
    "settings['Save_AVs_pkl']=True #Only used if Do_AV=True, saves AVs into a directory as appropriate for reloading\n",
    "settings['Save_AVs_xyz']=True #only used if Do_AV=True, saves AV xyz files for visualization\n",
    "settings['AV_label_atom']='CA' #must be an atom all residues have. can add exceptions later\n",
    "settings['Considered_Atoms']='Local_Labeled_Only' #which atoms to account for in AV simulations. An advanced feature to prevent side chains over-influencing AVs\n",
    "                                            #can be 'all' - , \n",
    "                                            #'Global_Labeled_Only' to only account for the labeled atoms, temporarily deleting all others (similar to Go, 1-bead model)\n",
    "                                            #'Local_Labeled_Only', temporarily deleting atoms of labeled residue (except labeled atom) and neighboring +/-X residues\n",
    "                                            #'Backbone_Only', temporarily deletes all non-backbone atoms\n",
    "settings['Residues_Range_to_Ignore']=1\n",
    "#below settings are applied as needed to residue positions\n",
    "settings['basic_dye_parameters']={\"Position\":\n",
    "    {\"Al488C5\": \n",
    "        {\"linker_length\": 20.5,\n",
    "            \"linker_width\": 2.5,\n",
    "            \"dye_radius1\": 5.0,\n",
    "            \"dye_radius2\": 4.5,\n",
    "            \"dye_radius3\": 1.5}, #\"attach_id\": int, attach id left blank as all pairs will have AVs computed, so this will be generated later\n",
    "    \"Al647C2\": \n",
    "        {\"linker_length\": 21.0,\n",
    "            \"linker_width\": 2.5,\n",
    "            \"dye_radius1\": 7.15,\n",
    "            \"dye_radius2\": 4.5,\n",
    "            \"dye_radius3\": 1.5}, #\"attach_id\": int, attach id left blank as all pairs will have AVs computed, so this will be generated later\n",
    "    },\n",
    "    \"Distance\": {\"Al488C5-Al647C2\": \n",
    "        {\"R0\": 52.,\n",
    "        \"n_dist\":10000}\n",
    "    }\n",
    "}\n",
    "\n",
    "settings['N_AV_points_mode'] = 'all' #all, average, or none\n",
    "settings['N_AV_points_threshold'] = 200\n",
    "\n",
    "\n",
    "if settings['Save_AVs_pkl']==True or settings['Save_AVs_xyz']==True:\n",
    "    avfoldername='AVs_{0}'.format(settings['Considered_Atoms'])\n",
    "    if settings['Considered_Atoms']=='Local_Labeled_Only':\n",
    "        avfoldername=avfoldername+'_'+str(settings['Residues_Range_to_Ignore'])+'_residue_window'\n",
    "    try:\n",
    "        os.mkdir(settings['output_directory']+avfoldername)\n",
    "    except FileExistsError:\n",
    "        print('AVs Directory exists')\n",
    "    except:\n",
    "        print('Something went wrong.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Import statements\n",
    "\n",
    "#import subprocess       #used for running bash within Python\n",
    "import numpy as np      #used for various math functions as well as arrays\n",
    "import math as m        #math functions\n",
    "import pandas as pd     #structure output files into useful formats\n",
    "import matplotlib.pyplot as plt #used for output plots\n",
    "from matplotlib.colors import LogNorm #useful normalization for many plots' colorscaling\n",
    "import matplotlib.colors as mcolor\n",
    "from itertools import combinations #used for generating pairwise combinations of objects, especially residues\n",
    "#import multiprocessing as mup #for future implementation of parallelization\n",
    "import json             #used for handling options files for AV sims. Will likely convert these to pandas objects\n",
    "\n",
    "import mdtraj as md     #used for loading and working with structure files. Alternative packages, with some reworking, would include Bio.PDB\n",
    "#import avtraj as av     #used for generating accessible volume simulations of fluorophores. However, gives warnings with numba\n",
    "import fretraj as ft    #alternative used for generating accessible volume simulations of fluorophores. Can also simulate photon traces\n",
    "import copy\n",
    "import warnings\n",
    "import random\n",
    "random.seed(settings['Randomization Seed'])\n",
    "\n",
    "from Script_Dependancies import GenerateMSA as msa\n",
    "\n",
    "\n",
    "os.chdir(settings['dependancy_directory']) #folder containing the scripts needed for this to run\n",
    "import DCA              #used for direct coupling analysis (DCA), a co-evolution approach to parsing out mutual information between residues that hints at interactions\n",
    "import ConservationScore #used for single residue conservation score analysis with simple shannon entropy. can add other function options as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fretraj: 0.2.10\n",
      "numpy: 1.24.4\n",
      "numba: 0.58.1\n",
      "matplotlib: 3.7.4\n",
      "python: 3.8.0 | packaged by conda-forge | (default, Nov 22 2019, 19:11:38) \n",
      "[GCC 7.3.0]\n",
      "pandas: 2.0.3\n",
      "json: 2.0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"fretraj: \"+ ft.__version__)\n",
    "print(\"numpy: \" + np.__version__)\n",
    "import numba as nb\n",
    "print(\"numba: \" + nb.__version__)\n",
    "import matplotlib as mpl\n",
    "print(\"matplotlib: \" + mpl.__version__)\n",
    "import sys\n",
    "print(\"python: \" + sys.version)\n",
    "print(\"pandas: \" + pd.__version__)\n",
    "# print(\"mdtraj: \" + md.__version__)\n",
    "print(\"json: \" + json.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x,u,s,mode='normal'):\n",
    "    if mode=='normal':\n",
    "        return np.exp(-.5*((x-u)/s)**2.)*(2*s**2.*np.pi)**(-.5)\n",
    "    if mode=='peakto1':\n",
    "        return np.exp(-.5*((x-u)/s)**2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add multi-chain support\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust figure output parameters for figures made in matplotlib\n",
    "import matplotlib.pylab as pylab \n",
    "pylab.rcParams.update(settings['figure_parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to or add input directory\n",
    "try:\n",
    "    os.chdir(settings['input_directory'])\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(settings['input_directory'])\n",
    "    os.chdir(settings['input_directory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas DataFrame will contain all single-residue information for each lookup\n",
    "single_residue_info=pd.DataFrame()\n",
    "#load the input structures\n",
    "md_struct=md.load(settings['input_structures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up initial single-residue information\n",
    "\n",
    "\n",
    "#grab residue list from structure. Maintains residue object types\n",
    "single_residue_info['Residue List']=md_struct.topology._residues\n",
    "#residue indices\n",
    "single_residue_info['Residue Index']=[x.resSeq for x in single_residue_info['Residue List']]\n",
    "#Residue 1 letter codes\n",
    "single_residue_info['One Letter Code']=[x for x in md_struct.topology.to_fasta()[0]]\n",
    "#Residue label atom Serial ID\n",
    "single_residue_info['{0} Serial ID'.format(settings['AV_label_atom'])]=[md_struct[0].top._atoms[i].serial for i in md_struct[0].top.select(\"name {0}\".format(settings['AV_label_atom']))]\n",
    "#Residue non-serial id (index); a unique identified that doesn't change as atoms are deleted\n",
    "single_residue_info['{0} Index ID'.format(settings['AV_label_atom'])]=[md_struct[0].top._atoms[i].index for i in md_struct[0].top.select(\"name {0}\".format(settings['AV_label_atom']))]\n",
    "#list of residues \"good\"=True or \"bad\"=False. Residues converted to bad when some undesirable quality is found at the single residue level\n",
    "single_residue_info['Good List']=[True for res in range(len(single_residue_info['Residue List']))]\n",
    "print('#Good={0}'.format(sum(single_residue_info['Good List'])))\n",
    "#remove residues we specified for removal\n",
    "if settings['exclude_residues']:\n",
    "    single_residue_info.loc[[x in settings['excluded_residues'] for x in single_residue_info['Residue Index']],'Good List']=False\n",
    "    print('#Good={0}'.format(sum(single_residue_info['Good List'])))\n",
    "if settings['define_domains']:\n",
    "    for i in range(len(single_residue_info)):\n",
    "        if not any([single_residue_info['Residue Index'][i] in x for x in [list(range(y[1],y[2]+1)) for y in settings['domain_bounds']]]):\n",
    "            single_residue_info['Good List'][i]=False\n",
    "    print('#Good={0}'.format(sum(single_residue_info['Good List'])))\n",
    "#define the index of the first residue as a reference for those outputs that index at 0 or 1\n",
    "first_residue=int(str(list(md_struct.topology.residues)[0])[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create initial dataframe containing residue pair-pair information\n",
    "\n",
    "#Pandas DataFrame will contain all pair-wise residue information for each pair. \n",
    "#length Nx(N-1)/2 (no need to compute self-pair parameters/diagonal of matrix nor degenrate pairs (ie 1-5, 5-1)). \n",
    "#Data per column is 1D, should be squareformed for plotting\n",
    "pair_residue_info=pd.DataFrame()\n",
    "pair_residue_info['Residue Pair']=list(combinations(single_residue_info['Residue Index'],2)) #given as tuples with indexing as in single_residue_info (original indexing)\n",
    "pair_residue_info['Good List']=[True for i in range(len(pair_residue_info))]\n",
    "#make sure pairs span domains. so first, create list of all valid pairs\n",
    "explicitdomains=[list(range(y[1], y[2]+1)) for y in settings['domain_bounds']]\n",
    "for j in range(len(explicitdomains)):\n",
    "    for i in settings['excluded_residues']:\n",
    "        if i in explicitdomains[j]:\n",
    "            explicitdomains[j].remove(i)\n",
    "if settings['define_domains']:        \n",
    "    domaincombos=combinations(list(range(len(settings['domain_bounds']))),2)\n",
    "    valid_pairs=[]\n",
    "    for i in domaincombos:\n",
    "        valid_pairs.extend([(x,y) for x in explicitdomains[i[0]] for y in explicitdomains[i[1]]])\n",
    "    for i in range(len(pair_residue_info)):\n",
    "        if pair_residue_info['Residue Pair'][i] not in valid_pairs:\n",
    "            pair_residue_info['Good List'][i]=False\n",
    "print('#Good pairs={0}'.format(sum(pair_residue_info['Good List'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for outputs\n",
    "\n",
    "try:\n",
    "    os.chdir(settings['output_directory'])\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(settings['output_directory'])\n",
    "    os.chdir(settings['output_directory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a text fasta file for input to external MSA script (header is also added to the file)\n",
    "with open('Input_Structure_FASTA.fasta','w+') as f:\n",
    "    f.write('>Input_Structure\\n'+md_struct.topology.to_fasta()[0]+'\\n')\n",
    "    f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto generate MSA alignment file when requested\n",
    "os.chdir('/zfs/smblab/fduffy/source/repos/FRETNet-Designer/')\n",
    "\n",
    "if  settings['generate_MSA'] == True:\n",
    "    msa.generate_msa(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while its called inDCA, this is the MSA file used for both DCA and single residue conservation\n",
    "#pregenerated should be made via Aish's get_alignments.bash until Frank's implementation is here\n",
    "\n",
    "if settings['generate_MSA'] == True:\n",
    "    inDCA=settings['outputMSA_filename']+'_filtered{0}'.format(settings['max_align_gaps'])\n",
    "else:\n",
    "    inDCA=settings['pregenerated_MSA_file']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform DCA using the generated multi sequence alignment\n",
    "#counterbalance to single residue conservation, as indicates variability in residue position may be dependent on another co-substitution\n",
    "if not settings['pregenerated_DCA']:    \n",
    "    outDCA=settings['DCA_output']\n",
    "    DCA.dca(inDCA,outDCA, theta=0.1)\n",
    "else:\n",
    "    outDCA=settings['pregenerated_DCA_file']\n",
    "DCAresult=np.loadtxt(outDCA)\n",
    "pair_residue_info['DCA Mutual Info']=DCAresult[:,2]\n",
    "pair_residue_info['DCA Direct Info']=DCAresult[:,3]\n",
    "del(DCAresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize DCA. Threshold here not applied in analysis, ony visualization. For analysis, DI values are instead ranked\n",
    "\n",
    "to_im=np.zeros((len(single_residue_info),len(single_residue_info)))\n",
    "to_im_thresh=np.zeros((len(single_residue_info),len(single_residue_info)))\n",
    "vis_thresh=0.05\n",
    "min_dist=3\n",
    "for i in range(len(pair_residue_info)):\n",
    "    if abs(pair_residue_info['Residue Pair'][i][0]-pair_residue_info['Residue Pair'][i][1])>min_dist:\n",
    "        to_im[pair_residue_info['Residue Pair'][i][0]-1,pair_residue_info['Residue Pair'][i][1]-1]=pair_residue_info['DCA Direct Info'][i]\n",
    "        if pair_residue_info['DCA Direct Info'][i]>vis_thresh:\n",
    "            to_im_thresh[pair_residue_info['Residue Pair'][i][0]-1,pair_residue_info['Residue Pair'][i][1]-1]=1\n",
    "plt.imshow(to_im, vmin=pair_residue_info['DCA Direct Info'].min())\n",
    "plt.show()\n",
    "plt.imshow(to_im_thresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single residue conservation scoring\n",
    "#uses simple Shannon entropy metric for the information associated with each residue position\n",
    "#high information content makes labeling favorable, as the position naturally experiences heterogeneity, so substitutions are favorable\n",
    "#output file name\n",
    "settings['output_conservation_score_file']='OutConScore.dat'\n",
    "#alphabet of supported characters/residues for conservation score analysis. for now, is just native protein AAs+placeholder characters\n",
    "settings['conservation_alphabet']=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y','-']\n",
    "\n",
    "#converts the sequence-wise array to a column-wise list to fead to conservation scorer\n",
    "columnwise=[col for col in np.array([[res for res in seq] for seq in np.loadtxt(inDCA,dtype='str',usecols=0)[1::2]]).T]\n",
    "\n",
    "#function takes a single string of characters and computes shannon entropy, so have to feed it columns from MSA.\n",
    "#computes entropy weighted by the equally-weighted probability of all alphabet entries (1./22. for 22 entries)\n",
    "ConScores=[ConservationScore.ConservationScore(col,settings['conservation_alphabet'],weight=1.,logbase=m.e) for col in columnwise]\n",
    "single_residue_info['Conservation Scores']=ConScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate inter-residue contacts (distances). Used to threshold sites to remove those that make contacts with each other\n",
    "contacts,contactsindices=md.compute_contacts(md_struct)\n",
    "maxadder=np.ones((len(contacts),1))*np.max(contacts)\n",
    "contacts=np.concatenate((maxadder,maxadder,contacts),axis=1)\n",
    "contactsindices=np.concatenate((np.array([[0,1],[0,2]]),contactsindices))\n",
    "for i in range(len(pair_residue_info)-4):\n",
    "    if contactsindices[i][0]<contactsindices[i+1][0] and contactsindices[i][0]<len(single_residue_info)-1:\n",
    "        contacts=np.concatenate((contacts[:,i+1:],maxadder,maxadder,contacts[:,:i+1]),axis=1)\n",
    "        contactsindices=np.vstack((contactsindices[:i+1], np.array([[contactsindices[i][0]+1,contactsindices[i][0]+2],[contactsindices[i][0]+1,contactsindices[i][0]+3]]),contactsindices[i+1:]))\n",
    "contacts=np.concatenate((contacts,maxadder,maxadder,maxadder),axis=1)\n",
    "contactsindices=np.vstack((contactsindices, np.array([[contactsindices[-1][0]+1,contactsindices[-1][1]-1],[contactsindices[-1][0]+1,contactsindices[-1][1]],[contactsindices[-1][0]+2,contactsindices[-1][1]]])))\n",
    "\n",
    "la=0\n",
    "for i in settings['input_structures']:\n",
    "    pair_residue_info['Contacts '+i+' (nm)']=contacts[la]\n",
    "    la+=1\n",
    "del(la)\n",
    "pair_residue_info['Contacts']=[i for i in contacts.T]\n",
    "pair_residue_info['Mean Contacts']=np.mean(contacts,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute solvent-accessible surface area for all residues in all frames\n",
    "#is a measure of accessibility for labeling\n",
    "SASA=md.shrake_rupley(md_struct,mode='residue')\n",
    "la=0\n",
    "for i in settings['input_structures']:\n",
    "    single_residue_info['SASA ' + i + ' (nm^2)']=SASA[la]\n",
    "    la+=1\n",
    "single_residue_info['Mean SASA (nm^2)']=np.mean(SASA,axis=0)\n",
    "del(la)\n",
    "del(SASA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the dye parameter file required by frettraj from the settings corresponding to fluorophores in the settings section.\n",
    "#will be modified as structure is modified if anything but \"all\" was selected for the av considered atoms mode\n",
    "\n",
    "if settings['generate_dye_parameters_file']==True:\n",
    "    Dye_Params={'Position':{}}\n",
    "    Dye_Params['Distance']=settings['basic_dye_parameters']['Distance']\n",
    "    for i in range(len(single_residue_info)):\n",
    "        for j in settings['basic_dye_parameters']['Position']:\n",
    "            Dye_Params['Position']['{0}_'.format(single_residue_info['Residue List'][i])+j]=copy.copy(settings['basic_dye_parameters']['Position'][j])\n",
    "            Dye_Params['Position']['{0}_'.format(single_residue_info['Residue List'][i])+j]['attach_id']=int(single_residue_info['{0} Serial ID'.format(settings['AV_label_atom'])][i])\n",
    "    json_file=json.dumps(Dye_Params)\n",
    "    with open(\"Dye_Params.json\", \"w\") as outfile:\n",
    "        outfile.write(json_file)\n",
    "    Dye_Params_file='Dye_Params.json'\n",
    "elif settings['generate_dye_parameters_file']=='Load':\n",
    "    Dye_Params_file='Dye_Params.json'\n",
    "    with open(Dye_Params_file, 'r') as f:\n",
    "        Dye_Params=json.load(f)\n",
    "elif settings['generate_dye_parameters_file']=='Specify':\n",
    "    Dye_Params_file=settings['specified_dye_parameters_file']\n",
    "    with open(Dye_Params_file, 'r') as f:\n",
    "        Dye_Params=json.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#generate AVs for all sites\n",
    "#modifies the structure and labeling files based on considered_atoms if Do_AV is true to reduce steric clashes at the labeling sites\n",
    "#can also load previous run AVs for the same structures for faster runs\n",
    "#this version is the old behavior, where it relies on fixes implemented directly into frattraj's cloud.py file\n",
    "#moving toward fixing on our end in this script, but keeping previous behavior on our end in case they fix frettraj\n",
    "\n",
    "if settings['fretraj_fix_our_side']==False: \n",
    "    temp1=[] \n",
    "    temp2=[]\n",
    "    Mod_Dye_Params_file='Mod_'+Dye_Params_file\n",
    "    if settings['Do_AV']==True:\n",
    "        mod_struct=copy.copy(md_struct) #generate a modified structure based on the desired considerations for av simulations\n",
    "\n",
    "\n",
    "        if settings['Considered_Atoms']=='Global_Labeled_Only': #for these two, can do the modification ahead of time so we don't have to read/write so often\n",
    "            #also have to re-assign the attachment atom IDs s.t. they correspond to the new serialized indices\n",
    "            with open(Dye_Params_file,'r') as f:\n",
    "                Mod_Dye_Params=json.load(f)\n",
    "            mod_struct=copy.copy(md_struct.atom_slice(np.where([i.name==settings['AV_label_atom'] for i in mod_struct.top.atoms])[0]))\n",
    "\n",
    "            with open(Mod_Dye_Params_file, 'w') as f:\n",
    "                f.write(json.dumps(Mod_Dye_Params))\n",
    "            labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "\n",
    "        elif settings['Considered_Atoms']=='Backbone_Only':\n",
    "\n",
    "            #also have to re-assign the attachment atom IDs s.t. they correspond to the new serialized indices\n",
    "            with open(Dye_Params_file,'r') as f:\n",
    "                Mod_Dye_Params=json.load(f)\n",
    "\n",
    "            mod_struct=copy.copy(md_struct.atom_slice(np.where([i.is_backbone for i in mod_struct.top.atoms])[0]))\n",
    "            with open(Mod_Dye_Params_file, 'w') as f:\n",
    "                f.write(json.dumps(Mod_Dye_Params))    \n",
    "\n",
    "            labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "        #for i in range(87,90):#\n",
    "        for i in range(len(single_residue_info)):\n",
    "            print(single_residue_info['Residue List'].iloc[i])\n",
    "            if settings['Considered_Atoms']=='all':\n",
    "                labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "                AVs1=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0], labels,np.arange(len(md_struct)))\n",
    "                AVs2=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1], labels,np.arange(len(md_struct)))\n",
    "            elif settings['Considered_Atoms']=='Global_Labeled_Only' or settings['Considered_Atoms']=='Backbone_Only':\n",
    "                AVs1=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0], labels,np.arange(len(md_struct)))\n",
    "                AVs2=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1], labels,np.arange(len(md_struct)))\n",
    "\n",
    "            elif settings['Considered_Atoms']=='Local_Labeled_Only':\n",
    "                #have to do the modifications on a per-residue basis here, so must reload mod_struct each time\n",
    "                mod_struct=copy.copy(md_struct)\n",
    "                #also have to re-assign the attachment atom IDs s.t. they correspond to the new serialized indices, and do so for each new residue\n",
    "                with open(Dye_Params_file,'r') as f:\n",
    "                    Mod_Dye_Params=json.load(f)\n",
    "                tempresname=str(list(mod_struct.top.residues)[i])\n",
    "                if i < settings['Residues_Range_to_Ignore']:\n",
    "                    mod_struct=copy.copy(md_struct.atom_slice(np.where([((j.residue not in list(single_residue_info['Residue List'].iloc[0:i+1+settings['Residues_Range_to_Ignore']])) or (j.name=='CA' and j.residue==single_residue_info['Residue List'].iloc[i])) for j in md_struct.top.atoms])[0]))\n",
    "                else:\n",
    "                    mod_struct=copy.copy(md_struct.atom_slice(np.where([((j.residue not in list(single_residue_info['Residue List'].iloc[i-settings['Residues_Range_to_Ignore']:i+1+settings['Residues_Range_to_Ignore']])) or (j.name=='CA' and j.residue==single_residue_info['Residue List'].iloc[i])) for j in md_struct.top.atoms])[0]))\n",
    "\n",
    "                with open(Mod_Dye_Params_file, 'w') as f:\n",
    "                    f.write(json.dumps(Mod_Dye_Params))                 \n",
    "\n",
    "                labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "                AVs1=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0], labels,np.arange(len(md_struct)))\n",
    "                AVs2=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1], labels,np.arange(len(md_struct)))\n",
    "\n",
    "                del(tempresname)\n",
    "            temp1.append(AVs1)\n",
    "            temp2.append(AVs2)\n",
    "\n",
    "            with warnings.catch_warnings(record=True):\n",
    "                if settings['Save_AVs_pkl']==True:\n",
    "                    os.chdir(settings['output_directory']+avfoldername)\n",
    "                    try:\n",
    "                        ft.cloud.save_obj(r'AVs1_{0}.pkl'.format(single_residue_info['Residue List'][i]), AVs1)\n",
    "                    except:\n",
    "                        print('AV1 of {0} is empty in at least one frame, not saving'.format(single_residue_info['Residue List'][i]))\n",
    "                    try:\n",
    "                        ft.cloud.save_obj(r'AVs2_{0}.pkl'.format(single_residue_info['Residue List'][i]), AVs2)\n",
    "                    except:\n",
    "                        print('AV2 of {0} is empty in at least one frame, not saving'.format(single_residue_info['Residue List'][i]))\n",
    "                    os.chdir(settings['output_directory'])\n",
    "                if settings['Save_AVs_xyz']==True:\n",
    "                    os.chdir(settings['output_directory']+avfoldername)\n",
    "                    try:\n",
    "                        ft.cloud.save_acv_traj(r'AVs1_{0}.xyz'.format(single_residue_info['Residue List'][i]), AVs1, format='xyz')\n",
    "                    except:\n",
    "                        print('AV1 of {0} is empty in at least one frame, not saving'.format(single_residue_info['Residue List'][i]))\n",
    "                    try:\n",
    "                        ft.cloud.save_acv_traj(r'AVs2_{0}.xyz'.format(single_residue_info['Residue List'][i]), AVs2, format='xyz')   \n",
    "                    except:\n",
    "                        print('AV2 of {0} is empty in at least one frame, not saving'.format(single_residue_info['Residue List'][i]))\n",
    "                    os.chdir(settings['output_directory'])\n",
    "        # del(mod_struct)\n",
    "    elif settings['Do_AV']=='Load':\n",
    "        labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "        for i in range(len(single_residue_info)):\n",
    "            os.chdir(settings['output_directory']+avfoldername)\n",
    "            try:\n",
    "                AVs1=ft.cloud.load_obj(r'AVs1_{0}.pkl'.format(single_residue_info['Residue List'][i]))\n",
    "            except:\n",
    "                print('AV1 of {0} file does not exist, was either empty or never saved. Will append None.'.format(single_residue_info['Residue List'][i]))\n",
    "                AVs1=None\n",
    "            try:\n",
    "                AVs2=ft.cloud.load_obj(r'AVs2_{0}.pkl'.format(single_residue_info['Residue List'][i]))\n",
    "            except:\n",
    "                print('AV2 of {0} file does not exist, was either empty or never saved. Will append None.'.format(single_residue_info['Residue List'][i]))\n",
    "                AVs2=None\n",
    "            os.chdir(settings['output_directory'])\n",
    "            temp1.append(AVs1)\n",
    "            temp2.append(AVs2)\n",
    "\n",
    "    single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[0]+'_AVs']=temp1\n",
    "    single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[1]+'_AVs']=temp2\n",
    "    del(temp1)\n",
    "    del(temp2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#generate AVs for all sites\n",
    "#takes a bit and outputs a lot; can be silenced but isnt so correct atoms can be veridied for labeling\n",
    "\n",
    "#generate AVs for all sites\n",
    "#modifies the structure and labeling files based on considered_atoms if Do_AV is true to reduce steric clashes at the labeling sites\n",
    "#can also load previous run AVs for the same structures for faster runs\n",
    "#this version is the new behavior, where it relies on fixes implemented directly here rather than frettraj files\n",
    "#moving toward fixing on our end in this script, but keeping previous behavior on our end in case they fix frettraj\n",
    "\n",
    "\n",
    "\n",
    "if settings['fretraj_fix_our_side']==True:\n",
    "    temp1=[] \n",
    "    temp2=[]\n",
    "    Mod_Dye_Params_file='Mod_'+Dye_Params_file\n",
    "    if settings['Do_AV']==True:\n",
    "        mod_struct=copy.copy(md_struct) #generate a modified structure based on the desired considerations for av simulations\n",
    "\n",
    "\n",
    "        if settings['Considered_Atoms']=='Global_Labeled_Only': #for these two, can do the modification ahead of time so we don't have to read/write so often\n",
    "            #also have to re-assign the attachment atom IDs s.t. they correspond to the new serialized indices\n",
    "            with open(Dye_Params_file,'r') as f:\n",
    "                Mod_Dye_Params=json.load(f)\n",
    "            mod_struct=copy.copy(md_struct.atom_slice(np.where([i.name==settings['AV_label_atom'] for i in mod_struct.top.atoms])[0]))\n",
    "            for j in keys:\n",
    "                Mod_Dye_Params['Position'][j]['attach_id']=[k.index for k in mod_struct.top.atoms if k.serial==Mod_Dye_Params['Position'][j]['attach_id']][0]+1\n",
    "            \n",
    "            \n",
    "            with open(Mod_Dye_Params_file, 'w') as f:\n",
    "                f.write(json.dumps(Mod_Dye_Params))\n",
    "            labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "\n",
    "        elif settings['Considered_Atoms']=='Backbone_Only':\n",
    "\n",
    "            #also have to re-assign the attachment atom IDs s.t. they correspond to the new serialized indices\n",
    "            with open(Dye_Params_file,'r') as f:\n",
    "                Mod_Dye_Params=json.load(f)\n",
    "\n",
    "            mod_struct=copy.copy(md_struct.atom_slice(np.where([i.is_backbone for i in mod_struct.top.atoms])[0]))\n",
    "            keys=list(Mod_Dye_Params['Position'].keys())\n",
    "            for j in keys:\n",
    "                Mod_Dye_Params['Position'][j]['attach_id']=[k.index for k in mod_struct.top.atoms if k.serial==Mod_Dye_Params['Position'][j]['attach_id']][0]+1\n",
    "            \n",
    "            with open(Mod_Dye_Params_file, 'w') as f:\n",
    "                f.write(json.dumps(Mod_Dye_Params))    \n",
    "\n",
    "            labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "        #for i in range(87,90):#\n",
    "        for i in range(len(single_residue_info)):\n",
    "            print(single_residue_info['Residue List'].iloc[i])\n",
    "            if settings['Considered_Atoms']=='all' or settings['Considered_Atoms']=='Global_Labeled_Only' or settings['Considered_Atoms']=='Backbone_Only':\n",
    "                with open(Dye_Params_file,'r') as f:\n",
    "                    Mod_Dye_Params=json.load(f)\n",
    "                \n",
    "                Mod_Dye_Params['Position'][str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0]]['attach_id']=[k.index for k in mod_struct.top.atoms if k.serial==Mod_Dye_Params['Position'][str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0]]['attach_id']][0]+1\n",
    "                Mod_Dye_Params['Position'][str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1]]['attach_id']=[k.index for k in mod_struct.top.atoms if k.serial==Mod_Dye_Params['Position'][str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1]]['attach_id']][0]+1\n",
    "\n",
    "                with open(Mod_Dye_Params_file, 'w') as f:\n",
    "                    f.write(json.dumps(Mod_Dye_Params))  \n",
    "                labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "                AVs1=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0], labels,np.arange(len(md_struct)))\n",
    "                AVs2=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1], labels,np.arange(len(md_struct)))\n",
    "                \n",
    "            elif settings['Considered_Atoms']=='Local_Labeled_Only':\n",
    "                #have to do the modifications on a per-residue basis here, so must reload mod_struct each time\n",
    "                mod_struct=copy.copy(md_struct)\n",
    "                #also have to re-assign the attachment atom IDs s.t. they correspond to the new serialized indices, and do so for each new residue\n",
    "                with open(Dye_Params_file,'r') as f:\n",
    "                    Mod_Dye_Params=json.load(f)\n",
    "                tempresname=str(list(mod_struct.top.residues)[i])\n",
    "                if i < settings['Residues_Range_to_Ignore']:\n",
    "                    mod_struct=copy.copy(md_struct.atom_slice(np.where([((j.residue not in list(single_residue_info['Residue List'].iloc[0:i+1+settings['Residues_Range_to_Ignore']])) or (j.name=='CA' and j.residue==single_residue_info['Residue List'].iloc[i])) for j in md_struct.top.atoms])[0]))\n",
    "                else:\n",
    "                    mod_struct=copy.copy(md_struct.atom_slice(np.where([((j.residue not in list(single_residue_info['Residue List'].iloc[i-settings['Residues_Range_to_Ignore']:i+1+settings['Residues_Range_to_Ignore']])) or (j.name=='CA' and j.residue==single_residue_info['Residue List'].iloc[i])) for j in md_struct.top.atoms])[0]))\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #Mod_Dye_Params['Position'][j]['attach_id']=[k.index for k in mod_struct.top.atoms if k.serial==Mod_Dye_Params['Position'][j]['attach_id']][0]+1\n",
    "                Mod_Dye_Params['Position'][str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0]]['attach_id']=[k.index for k in mod_struct.top.atoms if k.serial==Mod_Dye_Params['Position'][str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0]]['attach_id']][0]+1\n",
    "                Mod_Dye_Params['Position'][str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1]]['attach_id']=[k.index for k in mod_struct.top.atoms if k.serial==Mod_Dye_Params['Position'][str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1]]['attach_id']][0]+1\n",
    "\n",
    "                with open(Mod_Dye_Params_file, 'w') as f:\n",
    "                    f.write(json.dumps(Mod_Dye_Params))                 \n",
    "\n",
    "                labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "                AVs1=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[0], labels,np.arange(len(md_struct)))\n",
    "                AVs2=ft.cloud.Volume.from_frames(mod_struct, str(single_residue_info['Residue List'][i])+'_'+list(settings['basic_dye_parameters']['Position'].keys())[1], labels,np.arange(len(md_struct)))\n",
    "\n",
    "                del(tempresname)\n",
    "            temp1.append(AVs1)\n",
    "            temp2.append(AVs2)\n",
    "\n",
    "            with warnings.catch_warnings(record=True):\n",
    "                if settings['Save_AVs_pkl']==True:\n",
    "                    os.chdir(settings['output_directory']+avfoldername)\n",
    "                    try:\n",
    "                        ft.cloud.save_obj(r'AVs1_{0}.pkl'.format(single_residue_info['Residue List'][i]), AVs1)\n",
    "                    except:\n",
    "                        print('AV1 of {0} is empty in at least one frame, not saving'.format(single_residue_info['Residue List'][i]))\n",
    "                    try:\n",
    "                        ft.cloud.save_obj(r'AVs2_{0}.pkl'.format(single_residue_info['Residue List'][i]), AVs2)\n",
    "                    except:\n",
    "                        print('AV2 of {0} is empty in at least one frame, not saving'.format(single_residue_info['Residue List'][i]))\n",
    "                    os.chdir(settings['output_directory'])\n",
    "                if settings['Save_AVs_xyz']==True:\n",
    "                    os.chdir(settings['output_directory']+avfoldername)\n",
    "                    try:\n",
    "                        ft.cloud.save_acv_traj(r'AVs1_{0}.xyz'.format(single_residue_info['Residue List'][i]), AVs1, format='xyz')\n",
    "                    except:\n",
    "                        print('AV1 of {0} is empty in at least one frame, not saving'.format(single_residue_info['Residue List'][i]))\n",
    "                    try:\n",
    "                        ft.cloud.save_acv_traj(r'AVs2_{0}.xyz'.format(single_residue_info['Residue List'][i]), AVs2, format='xyz')   \n",
    "                    except:\n",
    "                        print('AV2 of {0} is empty in at least one frame, not saving'.format(single_residue_info['Residue List'][i]))\n",
    "                    os.chdir(settings['output_directory'])\n",
    "        # del(mod_struct)\n",
    "    elif settings['Do_AV']=='Load':\n",
    "        labels=ft.cloud.labeling_params(Mod_Dye_Params_file, verbose=False)\n",
    "        for i in range(len(single_residue_info)):\n",
    "            os.chdir(settings['output_directory']+avfoldername)\n",
    "            try:\n",
    "                AVs1=ft.cloud.load_obj(r'AVs1_{0}.pkl'.format(single_residue_info['Residue List'][i]))\n",
    "            except:\n",
    "                print('AV1 of {0} file does not exist, was either empty or never saved. Will append None.'.format(single_residue_info['Residue List'][i]))\n",
    "                AVs1=None\n",
    "            try:\n",
    "                AVs2=ft.cloud.load_obj(r'AVs2_{0}.pkl'.format(single_residue_info['Residue List'][i]))\n",
    "            except:\n",
    "                print('AV2 of {0} file does not exist, was either empty or never saved. Will append None.'.format(single_residue_info['Residue List'][i]))\n",
    "                AVs2=None\n",
    "            os.chdir(settings['output_directory'])\n",
    "            temp1.append(AVs1)\n",
    "            temp2.append(AVs2)\n",
    "\n",
    "    single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[0]+'_AVs']=temp1\n",
    "    single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[1]+'_AVs']=temp2\n",
    "    del(temp1)\n",
    "    del(temp2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#check integrated density in AVs. Essentially total volume, checked so we make sure dyes can reorient freely\n",
    "\n",
    "temp1=[]\n",
    "temp2=[]\n",
    "\n",
    "for i in range(len(single_residue_info)):\n",
    "    temper1=[]\n",
    "    temper2=[]\n",
    "    for j in single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[0]+'_AVs'][i]:\n",
    "        try:\n",
    "            temper1.append(sum(j.acv.grid_1d))\n",
    "            #temper2.append(sum(j.acv.cloud_xyzqt[:,4]))\n",
    "        except:\n",
    "            temper1.append(0)\n",
    "    for j in single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[1]+'_AVs'][i]:\n",
    "        try:\n",
    "            temper2.append(sum(j.acv.grid_1d))\n",
    "            #temper2.append(sum(j.acv.cloud_xyzqt[:,4]))\n",
    "        except:\n",
    "            temper2.append(0)\n",
    "    temp1.append(np.array(temper1))\n",
    "    temp2.append(np.array(temper2))\n",
    "\n",
    "single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[0]+'_grid1dsum']=temp1\n",
    "single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[1]+'_grid1dsum']=temp2\n",
    "single_residue_info['avg_'+list(settings['basic_dye_parameters']['Position'].keys())[0]+'_grid1dsum']=[np.mean(i) for i in temp1]\n",
    "single_residue_info['avg_'+list(settings['basic_dye_parameters']['Position'].keys())[1]+'_grid1dsum']=[np.mean(i) for i in temp2]\n",
    "del(temp1)\n",
    "del(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#threshold the above densities to remove sites with steric constraints on AVs\n",
    "if settings['N_AV_points_mode'] == 'all':\n",
    "    for i in range(len(single_residue_info)):\n",
    "        if any(single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[0]+'_grid1dsum'][i]<settings['N_AV_points_threshold']):\n",
    "            single_residue_info['Good List'][i] = False\n",
    "            #print('setting {0} false'.format(i))\n",
    "            \n",
    "elif settings['N_AV_points_mode'] == 'average':\n",
    "    for i in range(len(single_residue_info)):\n",
    "        if np.mean(single_residue_info[list(settings['basic_dye_parameters']['Position'].keys())[0]+'_grid1dsum'][i])<settings['N_AV_points_threshold']:\n",
    "            single_residue_info['Good List'][i] = False\n",
    "\n",
    "else:\n",
    "    print('Skipping elimination based on number of AV points')\n",
    "print('#Good={0}'.format(sum(single_residue_info['Good List'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of AV volumes\n",
    "plt.hist([sum(j.acv.grid_1d) for j in [i[0] for i in single_residue_info['Al488C5_AVs']]], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make sure that both residues in each pair are still in the single residue \"good list\"\n",
    "for i in range(len(pair_residue_info)):\n",
    "    if not (pair_residue_info['Residue Pair'][i][0] in single_residue_info[single_residue_info['Good List']]['Residue Index'].tolist()):\n",
    "        pair_residue_info['Good List'][i] = False\n",
    "        continue\n",
    "    if not (pair_residue_info['Residue Pair'][i][1] in single_residue_info[single_residue_info['Good List']]['Residue Index'].tolist()):\n",
    "        pair_residue_info['Good List'][i] = False\n",
    "print('#Good={0}'.format(sum(single_residue_info['Good List'])))\n",
    "print('#Good pairs={0}'.format(sum(pair_residue_info['Good List'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#do fret efficiency or distance metric calculations. Also calculate some additional metrics frettraj doesn't do by default\n",
    "#only does the calculations as dye1-dye2:res1-res2 for now, but could incorporate directed calculations easily as all AVs calculated already\n",
    "#can add option to only do attachment atom distances if FRET info not available or looking to do, say, DEER EPR\n",
    "\n",
    "tempFRETs=[]\n",
    "tempRmp_vec=[] #vectorial Rmp between AV clouds, calculated as RAV2-RAV1\n",
    "tempRatt_vec=[] #same, but between attachment atoms\n",
    "\n",
    "for i in range(len(pair_residue_info)):    \n",
    "    if pair_residue_info['Good List'][i]==True:\n",
    "        tempFRETs.append(ft.cloud.FRET.from_volumes(single_residue_info['{0}_AVs'.format(list(settings['basic_dye_parameters']['Position'].keys())[0])][single_residue_info['Residue Index']==pair_residue_info['Residue Pair'][i][0]].iloc[0],single_residue_info['{0}_AVs'.format(list(settings['basic_dye_parameters']['Position'].keys())[1])][single_residue_info['Residue Index']==pair_residue_info['Residue Pair'][i][1]].iloc[0],list(settings['basic_dye_parameters']['Distance'].keys())[0],labels))    \n",
    "        tempRmp_vec.append([k.acv.mp-j.acv.mp for j, k in zip(single_residue_info['{0}_AVs'.format(list(settings['basic_dye_parameters']['Position'].keys())[0])][single_residue_info['Residue Index']==pair_residue_info['Residue Pair'][i][0]].iloc[0], single_residue_info['{0}_AVs'.format(list(settings['basic_dye_parameters']['Position'].keys())[0])][single_residue_info['Residue Index']==pair_residue_info['Residue Pair'][i][1]].iloc[0])])\n",
    "        tempRatt_vec.append([k.av.attach_xyz-j.av.attach_xyz for j, k in zip(single_residue_info['{0}_AVs'.format(list(settings['basic_dye_parameters']['Position'].keys())[0])][single_residue_info['Residue Index']==pair_residue_info['Residue Pair'][i][0]].iloc[0], single_residue_info['{0}_AVs'.format(list(settings['basic_dye_parameters']['Position'].keys())[0])][single_residue_info['Residue Index']==pair_residue_info['Residue Pair'][i][1]].iloc[0])])\n",
    "    else:\n",
    "        tempFRETs.append(None)  \n",
    "        tempRmp_vec.append(None)  \n",
    "        tempRatt_vec.append(None)  \n",
    "pair_residue_info['{0}-{1}_FRET'.format(list(settings['basic_dye_parameters']['Position'].keys())[0],list(settings['basic_dye_parameters']['Position'].keys())[1])]=tempFRETs\n",
    "pair_residue_info['Rmp_vec']=tempRmp_vec\n",
    "pair_residue_info['Ratt_vec']=tempRatt_vec\n",
    "allEs=[]\n",
    "allRDAs=[]\n",
    "for i in tempFRETs:\n",
    "    if i==None:\n",
    "        allEs.append(None)\n",
    "        allRDAs.append(None)\n",
    "    else:\n",
    "        allEs.append(ft.cloud.Trajectory(i, timestep=1).dataframe['<E_DA>'])\n",
    "        allRDAs.append(ft.cloud.Trajectory(i, timestep=1).dataframe['<R_DA> (A)'])\n",
    "pair_residue_info['AllEs']=allEs\n",
    "pair_residue_info['AllRDAs']=allRDAs\n",
    "del(tempFRETs)\n",
    "del(tempRmp_vec)\n",
    "del(tempRatt_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now, add columns for the Rmp, R_DA, E, etc. and all the individual FRET eff params for distances corresponding to individual  states\n",
    "\n",
    "dist_types=['mean_R_DA', 'sigma_R_DA', 'mean_E_DA', 'sigma_E_DA', 'mean_R_DA_E', 'sigma_R_DA_E', 'R_attach', 'R_mp']\n",
    "#sigma is the std of the sampled distances WITHIN A GIVEN STATE FROM THAT STATES AV CLOUD\n",
    "tempdists={i:[] for i in dist_types}\n",
    "\n",
    "#these stds are the stds of the means amongst all states (how much the metric changes as conformation changes)\n",
    "for i in dist_types:\n",
    "    tempdists['std_'+i]=[]\n",
    "\n",
    "for i in range(len(pair_residue_info)):\n",
    "    if pair_residue_info['Good List'][i]==True:\n",
    "        for j in dist_types:\n",
    "            temp=[eval('i.{0}'.format(j)) for i in pair_residue_info['{0}-{1}_FRET'.format(list(settings['basic_dye_parameters']['Position'].keys())[0],list(settings['basic_dye_parameters']['Position'].keys())[1])][i]]\n",
    "            tempdists[j].append(temp)\n",
    "            tempdists['std_'+j].append(np.std(temp))\n",
    "    else:\n",
    "        for j in dist_types:\n",
    "            tempdists[j].append(None)\n",
    "            tempdists['std_'+j].append(None)\n",
    "            \n",
    "for i in tempdists:\n",
    "    pair_residue_info[i]=tempdists[i]\n",
    "del(tempdists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some distance contrast metrics since what we're interested in isn't distances but contrast in dists\n",
    "#between states\n",
    "\n",
    "def Dynamic_Shift(E1, E2):\n",
    "    return 2.**(-.5)*((1.-E1)**.5-(1.-E2)**.5)**2.\n",
    "\n",
    "def Contrast_Fun(a, b, mode='Dynamic_Shift', scale=1.):\n",
    "    if mode=='Dynamic_Shift':\n",
    "        con=Dynamic_Shift(a,b)\n",
    "    if mode=='Energy_Gaussian':\n",
    "        con=gaussian(a-b, 0, scale)\n",
    "    if mode=='Energy_InverseR':\n",
    "        con=scale*abs(a-b)**-1.\n",
    "    if mode=='Percent_Diff':\n",
    "        con=abs(a-b)/(a+b)*2\n",
    "    return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute representative stats for changes in distance metrics\n",
    "#will be used for ranking of sites in terms of \"fret contrast\"\n",
    "\n",
    "contrast_types=['mean_R_DA', 'mean_E_DA', 'mean_R_DA_E', 'R_attach', 'R_mp']\n",
    "custom_contrast=['Dynamic_Shift', 'Energy_InverseR', 'Energy_Gaussian'] #must be None, or some combo of 'Dynamic_Shift', 'Energy_InverseR', and 'Energy_Gaussian' in a list\n",
    "\n",
    "tempcons={'d_'+i:[] for i in contrast_types}\n",
    "#have to handle the dot product and dynamic shift bits separately\n",
    "tempcons['d_Rmp_vec']=[]\n",
    "tempcons['d_Ratt_vec']=[]\n",
    "#dot product is absolute value as we just want to see ANY change in Rda wrt Rattach\n",
    "tempcons['dRmpDOTdRatt']=[]\n",
    "if custom_contrast is not None:\n",
    "    for j in custom_contrast:\n",
    "        tempcons[j]=[]\n",
    "\n",
    "#this is the average of the contrast metric differences between frames\n",
    "temp_avgdifs={'avg_d_'+i:[] for i in contrast_types}\n",
    "temp_avgdifs['avg_dRmpDOTdRatt']=[]\n",
    "if custom_contrast is not None:\n",
    "    for j in custom_contrast:\n",
    "        temp_avgdifs['avg_'+j]=[]\n",
    "\n",
    "for i in range(len(pair_residue_info)):\n",
    "    if pair_residue_info['Good List'][i]==True:\n",
    "        for j in contrast_types:\n",
    "            temp=[abs(l-k) for k, l in combinations(pair_residue_info[j][i], 2)]\n",
    "            tempcons['d_'+j].append(temp)\n",
    "            temp_avgdifs['avg_d_'+j].append(np.mean(temp))\n",
    "        tempcons['d_Rmp_vec'].append([l-k for k, l in combinations(pair_residue_info['Rmp_vec'][i], 2)])\n",
    "        tempcons['d_Ratt_vec'].append([l-k for k, l in combinations(pair_residue_info['Ratt_vec'][i], 2)])\n",
    "        tempcons['dRmpDOTdRatt'].append([abs(np.sum(k*l))/(np.sum(k**2.)**.5)/(np.sum(l**2.)**.5) for k, l in zip(tempcons['d_Rmp_vec'][-1],tempcons['d_Ratt_vec'][-1])])\n",
    "        #tempcons['dRmpDOTdRatt'].append([np.dot(k/(np.sum(k**2.)**.5), (l/np.sum(l**2.)**.5)) for k, l in zip(tempcons['d_Rmp_vec'][-1],tempcons['d_Ratt_vec'][-1])])\n",
    "        temp_avgdifs['avg_dRmpDOTdRatt'].append(np.mean(tempcons['dRmpDOTdRatt'][-1]))\n",
    "        if custom_contrast is not None:\n",
    "            for m in custom_contrast:\n",
    "                tempcons[m].append([Contrast_Fun(k, l, m, scale=1./len(pair_residue_info['mean_E_DA'][i])) for k, l in combinations(pair_residue_info['mean_E_DA'][i], 2)])\n",
    "                temp_avgdifs['avg_'+m].append(np.mean(tempcons[m][-1]))    \n",
    "        \n",
    "    else:\n",
    "        for j in contrast_types:\n",
    "            tempcons['d_'+j].append(None)\n",
    "            temp_avgdifs['avg_d_'+j].append(None)\n",
    "        tempcons['d_Rmp_vec'].append(None)\n",
    "        tempcons['d_Ratt_vec'].append(None)\n",
    "        tempcons['dRmpDOTdRatt'].append(None)\n",
    "        temp_avgdifs['avg_dRmpDOTdRatt'].append(None)\n",
    "        if custom_contrast is not None:\n",
    "            for m in custom_contrast:\n",
    "                tempcons[m].append(None)\n",
    "                temp_avgdifs['avg_'+m].append(None)    \n",
    "\n",
    "for i in tempcons:\n",
    "    pair_residue_info[i]=tempcons[i]\n",
    "for i in temp_avgdifs:\n",
    "    pair_residue_info[i]=temp_avgdifs[i]\n",
    "        \n",
    "del(tempcons)\n",
    "del(temp_avgdifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#introduce a sequence proximity penalty for sites to avoid sites too close together\n",
    "\n",
    "proximity_penalty_scale=10.\n",
    "pair_residue_info['Sequence Proximity Penalty']=[gaussian(i[0]-i[1], 0, proximity_penalty_scale, 'peakto1') for i in pair_residue_info['Residue Pair']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score function definitions\n",
    "#just setting up some functions to be used in standardizing and ranking various other parameters\n",
    "\n",
    "def std_0to1(data):\n",
    "    return (data-np.min(data))/(np.max(data)-np.min(data))\n",
    "def std_Zscore(data):\n",
    "    return (data-np.mean(data))/np.std(data) #(x-u)/s\n",
    "def square(data):\n",
    "    return data**2.\n",
    "\n",
    "def score_parameters(data, weights=None, param_mod=None, standardize_data='0to1', standardize_score='0to1'):\n",
    "    #takes an input of a pd dataframe, outputs dataframe with same columns plus a summary score column\n",
    "    #data=input columns, as array of dataframe\n",
    "    #weights=optional weighting applied to each individual column in being added to the final score. must be same length as number of columns in data or None\n",
    "    #standardize options for whether to standardize input columns and score column. Supports '0to1' (min=0, max=1), 'Zscore' (value is number of stds away from mean), None (use raw data/output)\n",
    "    #param_mod should be either None or a list of length equal to the number of columns in data. List entries should be 'square' (square the data), 'abs' (absolute values of the data)\n",
    "    score=np.zeros(len(data))\n",
    "\n",
    "    \n",
    "    \n",
    "    if standardize_data=='0to1':\n",
    "        for i in data:\n",
    "            data[i]=std_0to1(data[i])\n",
    "    elif standardize_data=='Zscore':\n",
    "        for i in data:\n",
    "            data[i]=std_Zscore(data[i])        \n",
    "    \n",
    "    if weights is not None:\n",
    "        weights=pd.DataFrame(np.array(weights)[np.newaxis,:], columns=data.columns)\n",
    "    else:\n",
    "        weights=pd.DataFrame(np.ones((1,len(data.columns))),columns=data.columns)\n",
    "    \n",
    "    for i in data:\n",
    "        score=score+weights[i][0]*data[i]\n",
    "        \n",
    "    data['Score']=score\n",
    "    if standardize_score=='0to1':\n",
    "        data['Score']=std_0to1(data['Score'])\n",
    "    elif standardize_score=='Zscore':\n",
    "        data['Score']=std_Zscore(data['Score'])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need a cleanup step, as calculations thus far haven't considered favorability of parameters. \n",
    "#here, parameters are rephrased in terms of high score=bad\n",
    "#this is so the whole thing becomes a minimization problem\n",
    "#example, low solvent accessibility is bad, so multiply by -1, before standardizing. now values near 0 initially are highest\n",
    "\n",
    "pair_params_to_inverse=['avg_d_mean_R_DA', 'avg_d_mean_E_DA', 'avg_d_mean_R_DA_E', 'avg_d_R_attach', 'avg_d_R_mp', 'avg_dRmpDOTdRatt', 'avg_Dynamic_Shift', 'std_mean_R_DA', 'std_mean_E_DA', 'std_mean_R_DA_E', 'std_R_attach', 'std_R_mp', 'Mean Contacts', 'avg_Energy_InverseR', 'avg_Energy_Gaussian']\n",
    "single_params_to_inverse=['Mean SASA (nm^2)']\n",
    "for i in settings['basic_dye_parameters']['Position'].keys():\n",
    "    single_params_to_inverse.append('avg_'+i+'_grid1dsum')\n",
    "    \n",
    "for i in pair_params_to_inverse:\n",
    "    pair_residue_info[i]=-pair_residue_info[i]\n",
    "for i in single_params_to_inverse:\n",
    "    single_residue_info[i]=-single_residue_info[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate pair residue scores and individual residue scores\n",
    "#should probably move to settings, these are effectively weights for the individual parameters that\n",
    "#have will be standardized, multiplied by the weight, and added together\n",
    "#also determines which parameters, in the end, are actually used\n",
    "\n",
    "pair_parameters_to_score=['DCA Direct Info', 'Mean Contacts', 'avg_Dynamic_Shift', 'avg_dRmpDOTdRatt','Sequence Proximity Penalty']\n",
    "pair_parameter_weights=[1., .2, 1., .3, 0.]\n",
    "single_parameters_to_score=['Conservation Scores', 'Mean SASA (nm^2)','avg_'+list(settings['basic_dye_parameters']['Position'].keys())[0]+'_grid1dsum']\n",
    "single_parameter_weights=[.1, .1, .1]\n",
    "pair_residue_info['Pair_Score']=score_parameters(pair_residue_info[pair_residue_info['Good List']==True][pair_parameters_to_score], weights=pair_parameter_weights, standardize_data='0to1')['Score']\n",
    "single_residue_info['Single_Score']=score_parameters(single_residue_info[single_residue_info['Good List']==True][single_parameters_to_score], weights=single_parameter_weights, standardize_data='0to1')['Score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pair_residue_info['Pair_Score'], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate total score for each pair\n",
    "#weights for relative contribution of single residue and pair residue information\n",
    "pair_vs_single_weights=[1.,.5,.5]\n",
    "Res1Score=[]\n",
    "Res2Score=[]\n",
    "for i in range(len(pair_residue_info)):\n",
    "    Res1Score.append(single_residue_info[single_residue_info['Residue Index']==pair_residue_info['Residue Pair'][i][0]]['Single_Score'].iloc[0])\n",
    "    Res2Score.append(single_residue_info[single_residue_info['Residue Index']==pair_residue_info['Residue Pair'][i][1]]['Single_Score'].iloc[0])\n",
    "\n",
    "pair_residue_info['Residue 1 Score']=Res1Score\n",
    "pair_residue_info['Residue 2 Score']=Res2Score\n",
    "pair_residue_info['Total Score']=score_parameters(pair_residue_info[pair_residue_info['Good List']==True][['Pair_Score', 'Residue 1 Score', 'Residue 2 Score']], weights=pair_vs_single_weights, standardize_score='0to1')['Score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an additional function used for selecting pairs. Essentially allows favoring coincidence of one of two residues\n",
    "#in two different fret pairs (good for logistics of labeling, minimizing distinct perturbations to samples\n",
    "#with different fret pairs), but disfavoring nearby pairs (otherwise, redundant information from FRET pairs)\n",
    "\n",
    "def CoProximity_Penalty(Pair1, Pair2, width, coincidence_boon=1., norm='peakto1'):\n",
    "    penalty=0.\n",
    "    difs=[i-j for i, j in zip(Pair1, Pair2)]\n",
    "    if difs==[0 for i in difs]:\n",
    "        penalty+=np.inf\n",
    "    for i in difs:\n",
    "        if i==0:\n",
    "            penalty-=coincidence_boon*gaussian(0,0,width,norm)\n",
    "        else:\n",
    "            penalty+=gaussian(i, 0, width, norm)\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do selection of top X complementary FRET pairs via forward greedy approach. Select best scorer first, then add additional ones until X pairs added\n",
    "#so top pair is the \"seed\" and additional ones added greedily\n",
    "#also some weights (coincidence_boon, comproximity width and penalty fun, dot product, etc) provided\n",
    "#dot prod used to increase orthogonality between selected pairs\n",
    "#will instead implement dot product of vectorized contrast metrics (less physical orthogonality, more\n",
    "#orthogonality in observed FRET values under the relevant structural changes)\n",
    "ranked_candidates=pair_residue_info[pair_residue_info['Good List']].sort_values('Total Score')\n",
    "\n",
    "Num_Pairs=10\n",
    "CoProximity_Penalty_Width=20. #penalty for two sites in complementary pairs being near each other, but non-identical (shared nodes are good, but not neighboring nodes)\n",
    "#for the following functions, max and min judge based on each pairs worst/best performance respectively, mean and sum based on average performance\n",
    "CoProximity_Penalty_fun=lambda x: np.max(x)     #can be changed to min, max, mean, sum depending on which value should be used to penalize the complentarity score based Co-Proximity of labeling sites\n",
    "Coincidence_Boon=.1\n",
    "dot_prod_penalty_fun=lambda x: np.max(x)     #can be changed to min, max, mean, sum depending on which value should be used to penalize the complentarity score based on alignment of Rmp vectors\n",
    "dot_prod_among_states=lambda x: np.max(x)     #can be changed to min, max, mean, sum depending on which value should be used as the dot product between pairs amongst the dot products calculated for each state \n",
    "CoProximityWeight=1. #weight applied to CoProximity\n",
    "DotProdWeight=1.\n",
    "\n",
    "\n",
    "CompscoreMethod=lambda x, y: np.add(x,y) #should be either sum or multiply. Should be sum because otherwise negatives get funky\n",
    "\n",
    "ScoreMethod=lambda x, y: np.add(x,y) #should be either sum (add score and compscore together, in which case ScoreWeight matters) or multiply (in which case compscore is a modifier to the original score, and weight doesnt matter. negatives fine if Total Score set to 0to1)\n",
    "ScoreWeight=2. #weight applied to the score\n",
    "\n",
    "\n",
    "keep_list=[]\n",
    "keep_list.append(ranked_candidates.iloc[0])\n",
    "while len(keep_list)<Num_Pairs:\n",
    "    best=(None, np.inf) #best additional pair based on the criteria tested for complementarity\n",
    "    for j in range(len(ranked_candidates)):\n",
    "        #no repeat pairs allowed\n",
    "        if ranked_candidates['Residue Pair'].iloc[j] in [k['Residue Pair'] for k in keep_list]:\n",
    "            continue\n",
    "        compscore=1.\n",
    "        #applies penalty based on co-incidence of labeling sites in-exactly with ones already in the list\n",
    "        compscore=CompscoreMethod(compscore, CoProximityWeight*CoProximity_Penalty_fun([CoProximity_Penalty(ranked_candidates['Residue Pair'].iloc[j], k['Residue Pair'], CoProximity_Penalty_Width, coincidence_boon=Coincidence_Boon) for k in keep_list]))\n",
    "        #normalized dot product calculations of candidate pair against all other pairs, and for all states\n",
    "        compscore=CompscoreMethod(compscore, DotProdWeight*dot_prod_penalty_fun([dot_prod_among_states([np.sum(m[0]*m[1])*sum(m[0]**2.)**(-.5)*sum(m[1]**2.)**(-.5) for m in l]) for l in [zip(ranked_candidates['Rmp_vec'].iloc[j], k['Rmp_vec']) for k in keep_list]]))\n",
    "        #incorporate in the pair's score itself, either additively or multiplicatively\n",
    "        compscore=ScoreMethod(compscore, ScoreWeight*ranked_candidates['Total Score'].iloc[j])\n",
    "        \n",
    "        if compscore<best[1]:\n",
    "            best=(j, compscore)\n",
    "            \n",
    "    keep_list.append(ranked_candidates.iloc[best[0]])\n",
    "    \n",
    "Final_Network=pd.concat(keep_list, axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the selected FRET pairs on a radial plot\n",
    "\n",
    "ax=plt.subplot(111,projection='polar')\n",
    "firstres=single_residue_info['Residue Index'].iloc[0]\n",
    "lastres=single_residue_info['Residue Index'].iloc[-1]\n",
    "pairlist=np.array([[i[0],i[1]] for i in Final_Network['Residue Pair']]).astype(float).T\n",
    "for i in Final_Network['Residue Pair']:\n",
    "    ax.plot((pairlist-firstres)/(lastres-firstres)*2*np.pi,[1,1])\n",
    "ax.set_rmax(1)\n",
    "ax.set_theta_zero_location(\"W\")\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_rticks([])\n",
    "plt.grid(visible=None)\n",
    "plt.thetagrids((pairlist.flatten()-firstres)/(lastres-firstres)*360, labels=pairlist.flatten().astype(int))\n",
    "plt.savefig('Network.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the top FRET pairs that were selected\n",
    "To_Save_List=Final_Network[['Residue Pair', 'DCA Direct Info', 'avg_Dynamic_Shift', 'Pair_Score', 'Residue 1 Score', 'Residue 2 Score', 'Total Score','AllEs','AllRDAs']]\n",
    "To_Save_List.to_csv('Pair_List.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the scores of selected pairs. Should see them appear near the min, but distribution will change sample-to-sample\n",
    "\n",
    "plt.plot(np.arange(len(pair_residue_info['Total Score'][pair_residue_info['Total Score'].notna()])),pair_residue_info['Total Score'][pair_residue_info['Total Score'].notna()].sort_values(), color='black', linewidth=1)\n",
    "\n",
    "indices=[]\n",
    "for i in Final_Network['Residue Pair']:\n",
    "    indices.append((pair_residue_info[pair_residue_info['Total Score'].notna()].sort_values('Total Score')['Residue Pair']).tolist().index(i))\n",
    "plt.scatter(indices, Final_Network['Total Score'], color='red')\n",
    "\n",
    "\n",
    "plt.ylabel('Total Score')\n",
    "plt.xlabel('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_residue_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_residue_info.to_csv('all_pair_info.csv')\n",
    "single_residue_info.to_csv('all_single_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_fret_dev",
   "language": "python",
   "name": "python_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
